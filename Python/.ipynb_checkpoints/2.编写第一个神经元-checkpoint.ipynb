{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 编写第一个神经元"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一个神经元\n",
    "\n",
    "假设我们有一个神经元，这个神经元有三个输入。在大多数情况下，当你在神经网络中初始化参数时，我们的网络将随机初始化权重，并将偏差设置为零。我们这样做的原因将在稍后变得显而易见。输入将是实际的训练数据或神经网络中前一层神经元的输出。我们现在只准备从值开始作为输入："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [1, 2, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每个输入还需要与其关联的权重。输入是我们传递到模型中以获得所需输出的数据，而权重是我们稍后将调整以获得这些结果的参数。权重是在训练阶段模型内部发生变化的值类型之一，以及在训练期间也发生变化的偏差。权重和偏差的值是经过“训练”的，它们是使模型实际工作（或不工作）的因素。我们先从现在的重量开始。比方说，索引0处的第一个输入（即1）的权重为0.2，第二个输入的权重为0.8，第三个输入的权值为-0.5。我们的输入和权重列表现在应该是："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [1, 2, 3]\n",
    "weights = [0.2, 0.8, -0.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们需要偏见。目前，我们正在用三个输入对单个神经元进行建模。因为我们建模的是单个神经元，所以我们只有一个偏差，因为每个神经元只有一个偏置值。\n",
    "偏置是一个额外的可调值，但与权重相反。我们将随机选择值2作为本例的偏差："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [1, 2, 3]\n",
    "weights = [0.2, 0.8, -0.5]\n",
    "bias = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个神经元将每个输入乘以该输入的权重，然后加上偏差。神经元所做的只是获取输入的分数，其中这些分数（权重）是可调整的参数，然后添加另一个可调整的变量-偏差-然后输出结果。到目前为止，我们的产量计算如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3\n"
     ]
    }
   ],
   "source": [
    "output = inputs[0] * weights[0] + inputs[1] * weights[1] + inputs[2] * weights[2] + bias\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhangchunyang/opt/anaconda3/lib/python3.8/site-packages/IPython/core/display.py:724: UserWarning: Consider using IPython.display.IFrame instead\n",
      "  warnings.warn(\"Consider using IPython.display.IFrame instead\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe width=\"100%\" height=\"720\" src=\"http://aimaksen.bslience.cn/nnf/Single%20Neuron%20(BKR%200026).mp4\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasetstation.eda.video import video\n",
    "\n",
    "video(\"http://aimaksen.bslience.cn/nnf/Single%20Neuron%20(BKR%200026).mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果我们有4个输入，而不是刚才显示的3个输入，我们可能需要做什么改变？在附加输入旁边，我们需要添加一个关联的权重，这个新输入将与之相乘。我们还将为这个新重量补足一个值。此数据的代码可以是："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [1.0, 2.0, 3.0, 2.5]\n",
    "weights = [0.2, 0.8, -0.5, 1.0]\n",
    "bias = 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把以上所有的结合到一起："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [1.0, 2.0, 3.0, 2.5]\n",
    "weights = [0.2, 0.8, -0.5, 1.0]\n",
    "bias = 2.0\n",
    "\n",
    "output = (\n",
    "    inputs[0] * weights[0]\n",
    "    + inputs[1] * weights[1]\n",
    "    + inputs[2] * weights[2]\n",
    "    + inputs[3] * weights[3]\n",
    "    + bias\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"100%\" height=\"720\" src=\"http://aimaksen.bslience.cn/nnf/Single%20Neuron%202%20(0027).mp4\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video(\"http://aimaksen.bslience.cn/nnf/Single%20Neuron%202%20(0027).mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一层神经元\n",
    "\n",
    "神经网络通常具有由多个神经元组成的层。层只不过是一群神经元。一个层中的每个神经元接受完全相同的输入-给该层的输入（可以是训练数据，也可以是前一层的输出），但包含自己的权重集和自己的偏差，产生自己独特的输出。该层的输出是这些输出中的每一个的集合-每个神经元一个。假设我们有一个场景，在一个层中有3个神经元和4个输入："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video(\"http://aimaksen.bslience.cn/nnf/Four%20Neurons%20(MXO%200028).mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将保持第一个神经元的初始4个输入和权重集与我们迄今为止使用的相同。我们将添加2组额外的、合成的权重和2个额外的偏置，以形成2个新神经元，在层中总共3个。该层的输出将是3个值的列表，而不仅仅是单个神经元的单个值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [1, 2, 3, 2.5]\n",
    "weights1 = [0.2, 0.8, -0.5, 1]\n",
    "weights2 = [0.5, -0.91, 0.26, -0.5]\n",
    "weights3 = [-0.26, -0.27, 0.17, 0.87]\n",
    "bias1 = 2\n",
    "bias2 = 3\n",
    "bias3 = 0.5\n",
    "outputs = [\n",
    "    # Neuron 1:\n",
    "    inputs[0] * weights1[0]\n",
    "    + inputs[1] * weights1[1]\n",
    "    + inputs[2] * weights1[2]\n",
    "    + inputs[3] * weights1[3]\n",
    "    + bias1,\n",
    "    # Neuron 2:\n",
    "    inputs[0] * weights2[0]\n",
    "    + inputs[1] * weights2[1]\n",
    "    + inputs[2] * weights2[2]\n",
    "    + inputs[3] * weights2[3]\n",
    "    + bias2,\n",
    "    # Neuron 3:\n",
    "    inputs[0] * weights3[0]\n",
    "    + inputs[1] * weights3[1]\n",
    "    + inputs[2] * weights3[2]\n",
    "    + inputs[3] * weights3[3]\n",
    "    + bias3,\n",
    "]\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这个代码中，我们有三组权重和三个偏差，它们定义了三个神经元。每个神经元都“连接”到相同的输入。不同之处在于每个神经元应用于输入的单独权重和偏差。这被称为完全连接的神经网络——当前层中的每个神经元都与前一层的每个神经元有连接。这是一种非常常见的神经网络类型，但应该注意的是，不需要完全连接这样的一切。在这一点上，我们只展示了一个只有很少神经元的单层代码。想象一下编码更多的层和更多的神经元。这对于使用我们当前的方法编写代码来说将是非常困难的。相反，我们可以使用循环来缩放和处理动态大小的输入和层。我们已经将单独的权重变量转换为一个权重列表，这样我们就可以对它们进行迭代，并将代码更改为使用循环而不是硬编码操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output of current layer\n",
    "layer_outputs = []\n",
    "# For each neuron\n",
    "for neuron_weights, neuron_bias in zip(weights, biases):\n",
    "    # Zeroed output of given neuron\n",
    "    neuron_output = 0\n",
    "    # For each input and weight to the neuron\n",
    "    for n_input, weight in zip(inputs, neuron_weights):\n",
    "        # Multiply this input by associated weight\n",
    "        # and add to the neuron's output variable\n",
    "        neuron_output += n_input * weight\n",
    "    # Add bias\n",
    "    neuron_output += neuron_bias\n",
    "    # Put neuron's result to the layer's output list\n",
    "    layer_outputs.append(neuron_output)\n",
    "print(layer_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这与以前一样，只是以一种更加动态和可伸缩的方式。如果您在其中一个步骤中感到困惑，请打印（）对象以查看它们是什么以及正在发生什么。\n",
    "zip（）函数允许我们同时迭代多个可迭代对象（在本例中为列表）。同样，我们所做的是，对于每个神经元（上面代码中的外循环，超过神经元权重和权重），将每个输入值乘以该输入的相关权重（上面代码的内循环，超过输入和权重）并将所有这些加在一起，然后在最后加上一个偏差。最后，将神经元的输出发送到层的输出列表。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "就是这样！我们怎么知道我们有三个神经元？为什么我们有三个？我们可以知道我们有三个神经元，因为有三组权重和三个偏差。当你建立自己的神经网络时，你还可以决定每个层需要多少神经元。你可以将你得到的输入与你想要的神经元结合起来。当你读完这本书时，你会对尝试使用多少神经元有一些直觉。我们将从使用少量神经元开始，以帮助理解神经网络在其核心是如何工作的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用上面使用循环的代码，我们可以将层中输入或神经元的数量修改为我们想要的数量，我们的循环可以处理它。正如我们前面所说的，如果不在这里显示NumPy，那将是一种伤害，因为Python本身不能非常有效地进行矩阵/张量/数组数学。但首先，Python中最流行的深度学习库被称为“TensorFlow”的原因是它都是关于对张量进行操作的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 张量、数组和向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "什么是“张量”？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "张量与数组密切相关。如果你在机器学习时交换张量/数组/矩阵，人们可能不会给你太多时间。但是有细微的差异，它们主要是张量对象的上下文或属性。为了理解张量，让我们比较并描述Python中的一些其他数据容器（保存数据的东西）。让我们从列表开始。Python列表由括号中包含的逗号分隔的对象定义。到目前为止，我们一直在使用列表。这是一个简单列表的示例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这是一个简单列表的示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [1, 5, 6, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A list of lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lol = [[1, 5, 6, 2], [3, 2, 1, 3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "列表的列表！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lolol = [\n",
    "    [[1, 5, 6, 2], [3, 2, 1, 3]],\n",
    "    [[5, 2, 1, 2], [6, 4, 8, 4]],\n",
    "    [[2, 8, 5, 3], [1, 1, 9, 4]],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "到目前为止所显示的一切也可以是张量的数组或数组表示。列表只是一个列表，它可以做任何它想做的事情，包括"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "another_list_of_lists = [[4, 2, 3], [5, 1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的列表列表不能是数组，因为它不是同源的。如果一个维度上的每个列表都相同长，那么列表列表是同源的，并且对于每个维度都必须如此。在上面显示的列表中，它是一个二维列表。第一个维度的长度是总列表中的子列表的数量（2）。第二个维度是每个子列表的长度（3，然后2）。在上面的例子中，当读取“行”维度（也称为第二维度）时，第一个列表有3个元素长，第二个列表有2个元素长-这不是同源的，因此不能是数组。虽然不能在一个维度上保持一致足以表明这个例子是不同源的，但我们也可以读出“列”维度（第一维度）；前两列为2个元素长，而第三列仅包含1个元素。注意，每个尺寸不一定需要相同的长度；具有4行3列（即4x3）的阵列是完全可以接受的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "矩阵非常简单。这是一个矩形阵列。它有列和行。它是二维的。\n",
    "因此，矩阵可以是阵列（2D阵列）。所有数组都可以是矩阵吗？不。数组可以远不止是列和行，因为它可以有四个维度，二十个维度，等等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_matrix_array = [[4, 2], [5, 1], [8, 2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的列表也可以是一个有效的矩阵（因为它的列和行），这意味着它也可以是数组。这个数组的“形状”将是3x2，或者更正式地描述为（3，2）的形状，因为它有3行2列。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了表示形状，我们需要检查每个维度。正如我们已经了解的，矩阵是一个二维数组。第一个维度是最外括号内的内容，如果我们看上面的矩阵，我们可以看到三个列表：[4,2]、[5,1]和[8,2]；因此，该维度中的大小为3，并且这些列表中的每一个必须具有相同的形状以形成阵列（在这种情况下为矩阵）。下一个维度的大小是这个更内部的一对括号内的元素数，我们看到它是2，因为它们都包含2个元素。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于三维数组，如下面的lolol，我们将有第三级括号："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lolol = [\n",
    "    [[1, 5, 6, 2], [3, 2, 1, 3]],\n",
    "    [[5, 2, 1, 2], [6, 4, 8, 4]],\n",
    "    [[2, 8, 5, 3], [1, 1, 9, 4]],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此数组的第一级包含3个矩阵："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[1, 5, 6, 2], [3, 2, 1, 3]][[5, 2, 1, 2], [6, 4, 8, 4]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[2, 8, 5, 3], [1, 1, 9, 4]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这就是最外括号内的内容，这个维度的大小是3。如果我们看第一个矩阵，我们可以看到它包含2个列表-[1,5,6,2]和[3,2,1,3]，所以这个维度的尺寸是2，而这个内矩阵的每个列表都包含4个元素。这4个元素构成了这个矩阵的第3维和最后一维，因为没有更多的内括号。\n",
    "因此，这个数组的形状是（3，2，4），它是一个三维数组，因为这个形状包含3个维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video(\"http://aimaksen.bslience.cn/nnf/Array%20Shape%20(JPS%200036).mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，什么是张量？当涉及到在计算机科学背景下讨论张量与数组时，一页又一页的争论接踵而至。这场激烈的辩论似乎是由人们从完全不同的地方争论的事实引起的。毫无疑问，张量不仅仅是一个数组，但真正的问题是：“在深度学习的背景下，对于计算机科学家来说，张量是什么？”我们相信，我们可以用一句话来解决这场辩论："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 张量对象是可以表示为数组的对象"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这意味着，作为程序员，我们可以（并且将）在深度学习的背景下将张量视为数组，这就是我们必须投入其中的全部思想。所有张量都是数组吗？不，但它们在我们的代码中表示为数组，所以，对我们来说，它们只是数组，这就是为什么会有这么多的争论和混乱。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，什么是数组？在本书中，我们将数组定义为数字的有序同源容器，并且在使用NumPy包时通常使用这个术语，因为这是其中的主要数据结构。线性数组，也称为一维数组，是数组的最简单示例，在普通Python中，这将是一个列表。数组也可以由多维数据组成，最著名的例子之一是我们在数学中称之为矩阵，我们将其表示为二维数组。可以使用索引元组作为键访问数组的每个元素，这意味着我们可以检索任何数组元素。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们需要再学习一个概念——向量。简单地说，数学中的向量就是我们在Python中称为列表的向量，或者在NumPy中称为一维数组的向量。当然，列表和NumPy数组没有与向量相同的属性，但是，正如我们可以在Python中将矩阵作为列表编写一样，我们也可以将向量作为列表或数组编写！此外，我们将从代数角度（数学上）将向量视为括号中的一组数字。这与物理观点形成对比，在物理观点中，向量的表示通常被视为一个箭头，以大小和方向为特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 点积和向量相加"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们来讨论向量乘法，因为这是我们将对向量执行的最重要的操作之一。我们可以获得与纯Python实现中相同的结果，即使用点积将输入中的每个元素相乘，并按元素对向量进行加权，我们稍后将对此进行解释。“然而，这似乎增加了神经网络的神秘性——就像它们是我们永远无法理解的复杂多维向量空间中的对象一样。继续将向量视为数组——一维数组只是向量（或Python中的列表）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于变量和互连的数量之多，我们可以用非线性激活函数对非常复杂和非线性的关系进行建模，并真正感觉像是巫师，但这可能弊大于利。是的，我们将使用“点积”，但我们这样做是因为它以一种干净的方式执行必要的计算。没有比这更深入的了——正如你已经看到的，我们可以用更基本的单词来计算这个数学。乘向量时，可以执行点积或叉积。叉积产生向量，点积产生标量（单个值/数字）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，让我们解释一下两个向量的点积是什么。数学家会说："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center;\">\n",
    "    <img src=\"http://aimaksen.bslience.cn/nnf/dot-product-formula.jpg\">\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "两个矢量的点积是连续矢量元素的乘积之和。两个矢量必须具有相同的大小（具有相等数量的元素）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们写出点积是如何在Python中计算的。对于它，您有两个向量，我们可以在Python中将其表示为列表。然后，我们将它们的元素与相同的索引值相乘，然后将所有结果相加。假设我们有两个列表作为向量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 2, 3]\n",
    "b = [2, 3, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要获得点积："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_product = a[0] * b[0] + a[1] * b[1] + a[2] * b[2]\n",
    "print(dot_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video(\"http://aimaksen.bslience.cn/nnf/Dot%20Product%20(XPO%200029).mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，如果我们称a为“输入”，b为“权重”呢突然之间，这个点产品看起来像是一种简洁的方式来执行我们需要的操作，并且已经在普通Python中执行了。我们需要将权重和相同索引值的输入相乘，并将结果值相加。点积执行这种精确类型的操作；因此，在这里使用它很有意义。回到神经网络代码，让我们利用这个点积。普通Python不包含执行此类操作的方法或函数，因此我们将使用NumPy包，它能够执行此操作，以及我们将来将使用的更多操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们还需要在不久的将来执行向量加法运算。幸运的是，NumPy让我们以一种自然的方式执行这一操作——对包含数据向量的变量使用加号。两个矢量的相加是按元素执行的操作，这意味着两个矢量必须具有相同的大小，结果将成为尺寸也一样。结果是作为连续矢量元素的和计算的矢量："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center;\">\n",
    "    <img src=\"http://aimaksen.bslience.cn/vector-addition.jpg\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 Numpy 实现一个神经元"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们编写解决方案，从单个神经元开始，使用点积和矢量与NumPy的相加。这使得代码读写更简单（运行更快）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "inputs = [1.0, 2.0, 3.0, 2.5]\n",
    "weights = [0.2, 0.8, -0.5, 1.0]\n",
    "bias = 2.0\n",
    "outputs = np.dot(weights, inputs) + bias\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video(\"http://aimaksen.bslience.cn/nnf/Dot%20Product%20Neuron%20(BLQ%200030).mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 Numpy 实现一层神经元"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们回到一个点，我们想计算一层3个神经元的输出，这意味着权重将是一个矩阵或权重向量列表。在普通Python中，我们将其写为列表列表。使用NumPy，这将是一个二维数组，我们称之为矩阵。在之前的3神经元示例中，我们将这些权重与包含输入的列表相乘，从而生成输出值列表-每个神经元一个。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们还描述了两个向量的点积，但权重现在是一个矩阵，我们需要对它们和输入向量进行点积。NumPy让我们很容易做到这一点-将这个矩阵视为向量列表，并用输入向量逐个执行点积，返回点积列表。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在我们的例子中，点积的结果是每个神经元的权重和偏差积之和的向量（或列表）。从这里开始，我们仍然需要给它们添加相应的偏差。偏置可以很容易地添加到点积运算的结果中，因为它们是相同大小的向量。\n",
    "我们也可以在这里直接使用普通Python列表，因为NumPy会在内部将其转换为数组。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以前，我们通过执行点积并逐个加上偏差来计算每个神经元的输出。现在我们改变了这些操作的顺序-我们首先对所有神经元和输入执行点积操作，然后在下一个操作中添加一个偏差。当我们使用NumPy将两个向量相加时，每个第i个元素都相加在一起，得到一个相同大小的新向量。这既是一种简化，也是一种优化，为我们提供了更简单、更快的代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "inputs = [1.0, 2.0, 3.0, 2.5]\n",
    "weights = [[0.2, 0.8, -0.5, 1], [0.5, -0.91, 0.26, -0.5], [-0.26, -0.27, 0.17, 0.87]]\n",
    "biases = [2.0, 3.0, 0.5]\n",
    "layer_outputs = np.dot(weights, inputs) + biases\n",
    "print(layer_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video(\"http://aimaksen.bslience.cn/nnf/Dot%20Product%20Layer%20(CYX%200031).mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这种语法涉及权重和输入的点积，然后是偏置的矢量加法，是表示输入·权重+偏置的最常用方法。为了解释我们传递到np-dot（）中的参数的顺序，我们应该将其视为首先出现的将决定输出形状。在我们的例子中，我们首先传递神经元权重列表，然后传递输入，因为我们的目标是获得神经元输出列表。正如我们所提到的，矩阵和向量的点积会产生一系列点积。np-dot（）方法将矩阵视为向量列表，并将每个向量与另一个向量进行点积。在本例中，我们使用该属性传递一个矩阵，该矩阵是神经元权重向量和输入向量的列表，并获得一个点积-神经元输出的列表。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 批量数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了训练，神经网络倾向于成批地接收数据。到目前为止，示例输入数据只是被称为特征集的各种特征的一个样本（或观察）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [1, 2, 3, 2.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这里，[1，2，3，2.5] 数据在某种程度上对我们想要的输出有意义和描述性。将每个数字想象为来自不同传感器的值，来自第1章中的示例，全部同时。这些值中的每一个都是特征观测基准，它们一起形成了一个特征集实例，也称为观测，或者最常见的是样本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"100%\" height=\"720\" src=\"http://aimaksen.bslience.cn/nnf/Batch%20of%20Samples%200037.mp4\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video(\"http://aimaksen.bslience.cn/nnf/Batch%20of%20Samples%200037.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通常，神经网络希望一次采集许多样本，原因有两个。一个原因是在并行处理中分批训练更快，另一个原因则是分批训练有助于训练期间的概括。如果一次只对一个样本进行拟合（执行训练过程的一个步骤），则很可能会继续对单个样本进行拟合，而不是缓慢地对适合整个数据集的权重和偏差进行一般调整。批量试衣或训练让你更有机会对体重和偏差做出更有意义的改变。对于成批装配的概念，而不是一次一个样本，以下动画可以帮助您："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video(\"http://aimaksen.bslience.cn/nnf/Why%20To%20Use%20Batches%20(VYU%200040).mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一批数据的示例如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video(\"http://aimaksen.bslience.cn/nnf/Batch%20of%20Samples%200037.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回想一下，在Python中，在我们的例子中，列表是保存一个样本以及组成一批观察的多个样本的有用容器。这是一批观测的例子，每个观测都有自己的样本，如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [[1, 2, 3, 2.5], [2, 5, -1, 2], [-1.5, 2.7, 3.3, -0.8]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个列表列表可以做成一个数组，因为它是同源的。请注意，这个较大列表中的每个“列表”都是表示一个功能集的示例。[1、2、3、2.5]、[2、5、-1、2]和[-1.5、2.7、3.3、-0.8] 都是样本，也称为特征集实例或观察。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们现在有了一个输入矩阵和一个权重矩阵，我们需要以某种方式对它们进行点积运算，但结果是怎样的？类似地，当我们对矩阵和向量执行点积时，我们将矩阵视为向量列表，从而得到点积列表。在本例中，我们需要将这两个矩阵作为向量列表进行管理，并在所有组合中对所有矩阵执行点积，从而生成输出列表或矩阵列表；这种运算称为矩阵乘积。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 矩阵乘法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "矩阵乘积是一种运算，其中我们有2个矩阵，我们对第一个矩阵的行和第二个矩阵的列的所有组合执行点乘积，得到这些原子点乘积的矩阵："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video(\"http://aimaksen.bslience.cn/nnf/Matrix%20Product%20(JEI%200032).mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要执行矩阵乘积，左矩阵的第二维度的大小必须与右矩阵的第一维度的大小匹配。例如，如果左矩阵的形状为（5，4），则右矩阵必须在第一个形状值（4，7）内匹配此4。结果数组的形状始终是左数组的第一维度和右数组的第二维度（5，7）。在上例中，左矩阵的形状为（5，4），右上矩阵的形状是（4，5）。左数组的第二维度和第二数组的第一维度都是4，它们匹配，结果数组的形状为（5，5）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了详细说明，我们还可以证明我们可以对向量执行矩阵乘积。在数学中，我们可以有一个称为列向量和行向量的东西，稍后我们会更好地解释。它们是向量，但表示为矩阵，其中一个维度的大小为1："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center;\">\n",
    "    <img src=\"http://aimaksen.bslience.cn/vectors.jpg\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a是行向量。它看起来与前面描述的向量a（上面有箭头）以及向量乘积非常相似。行向量和向量之间的符号差异是值之间的逗号，并且行向量上缺少符号a上方的箭头。它被称为行向量，因为它是矩阵中一行的向量。b、 另一方面，它被称为列向量，因为它是矩阵的一列。由于行和列向量在技术上是矩阵，我们不再用向量箭头表示它们。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当我们对它们执行矩阵乘积时，结果也会变成矩阵，就像前面的例子一样，但只包含一个值，与我们前面讨论的点积例子中的值相同："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center;\">\n",
    "    <img src=\"http://aimaksen.bslience.cn/vectors-product.jpg\">\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"100%\" height=\"720\" src=\"http://aimaksen.bslience.cn/nnf/Matrix%20Product%20Inputs%20Weights%20(BKW%200033).mp4\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video(\n",
    "    \"http://aimaksen.bslience.cn/nnf/Matrix%20Product%20Inputs%20Weights%20(BKW%200033).mp4\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "换句话说，行向量和列向量是矩阵，其中一个维度的大小为1；并且，我们对它们执行矩阵乘积而不是点乘积，这导致矩阵包含单个值。在这种情况下，我们对形状为（1，3）和（3，1）的矩阵进行矩阵乘法，然后得到的数组具有形状（1，1）或大小为1x1。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 矩阵乘法的转置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们是如何突然从2个向量变成行和列向量的？我们使用点积和矩阵积的关系表示两个向量的点积等于行和列向量的矩阵积（字母上方的箭头表示它们是向量）："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center;\">\n",
    "    <img src=\"http://aimaksen.bslience.cn/matrix-t.jpg\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们还暂时使用了一些简化，没有显示列向量b实际上是一个转置向量b。匹配向量a和b的点积作为矩阵积的适当方程应该如下所示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center;\">\n",
    "    <img src=\"http://aimaksen.bslience.cn/matrix-t-prod.jpg\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里我们又介绍了一个新的操作——换位。Transposition简单地修改矩阵，使其行变为列，列变为行："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video(\"http://aimaksen.bslience.cn/nnf/Transposition%20%20(QUT%200002B).mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video(\"http://aimaksen.bslience.cn/nnf/Transposition%20(pnq%200002).mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们需要回到行和列向量定义，并用我们刚刚学到的内容更新它们。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "行向量是一个矩阵，其第一维度的大小（行数）等于1，第二维度的尺寸（列数）等于n-向量大小。换句话说，它是一个1×n数组或形状（1，n）数组："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center;\">\n",
    "    <img src=\"http://aimaksen.bslience.cn/a.jpg\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于NumPy和3个值，我们将其定义为："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([[1, 2, 3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请注意此处使用的双括号。要将列表转换为包含单行的矩阵（执行将向量转换为行向量的等效操作），我们可以将其放入列表并创建numpy数组："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 2, 3]\n",
    "print(np.array([a]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再次注意，在本例中，在转换为数组之前，我们将a括在括号中。\n",
    "或者我们可以将其转换为一维数组，并使用NumPy功能之一扩展维度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 2, 3]\n",
    "print(np.expand_dims(np.array(a), axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中，npexpand_dims（）在轴的索引处添加一个新维度。\n",
    "列向量是第二维大小等于1的矩阵，换句话说，它是形状（n，1）的数组："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center;\">\n",
    "    <img src=\"http://aimaksen.bslience.cn/b.jpg\">\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用NumPy，它可以以与行向量相同的方式创建，但需要额外转置-转置将行转换为列，将列转换为行："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center;\">\n",
    "    <img src=\"http://aimaksen.bslience.cn/b-t.jpg\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要将向量b转化为行向量b，我们将使用与我们将向量a转化为行矢量a相同的方法，然后我们可以对其进行转置，使其成为列向量b："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center;\">\n",
    "    <img src=\"http://aimaksen.bslience.cn/b-&-bt.jpg\">\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用NumPy代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = [1, 2, 3]\n",
    "b = [2, 3, 4]\n",
    "a = np.array([a])\n",
    "b = np.array([b]).T\n",
    "print(np.dot(a, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们获得了与两个向量的点积相同的结果，但对矩阵进行了运算，并返回了一个矩阵——这正是我们所期望和想要的。值得一提的是，NumPy没有专用的方法来执行矩阵乘积-点乘积和矩阵乘积都是在一个方法中实现的：np-dot（）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如我们所见，要对两个向量执行矩阵乘积，我们按原样取一个向量，将其转换为行向量，第二个向量使用转置将其转换成列向量。这使得我们可以执行一个矩阵乘积，返回一个包含单个值的矩阵。我们还对两个示例数组执行了矩阵乘积，以了解矩阵乘积的工作原理-它创建了行和列向量的所有组合的点乘积矩阵。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一层神经元和批量处理数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们回到我们的输入和权重-在覆盖它们时，我们提到我们需要对包含输入和权重矩阵的所有向量执行点积。正如我们刚刚学到的，这就是矩阵乘积的运算。我们只需要对它的第二个参数（在我们的例子中是权重矩阵）执行转置，将它当前包含的行向量转换为列向量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最初，我们能够在没有换位的情况下对输入和权重执行点积，因为权重是一个矩阵，但输入只是一个向量。在这种情况下，点积产生了一个由矩阵和这个单个向量的每一行执行的原子点积向量。当输入变成一批输入（矩阵）时，我们需要执行矩阵乘积。它从左矩阵中获取行和右矩阵中获取列的所有组合，对它们执行点积，并将结果放入输出数组中。两个数组都有相同的形状，但要执行矩阵乘积，第一个矩阵的索引1和第二个矩阵的指数0的形状值必须匹配-它们现在不匹配。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video(\n",
    "    \"http://aimaksen.bslience.cn/nnf/Why%20To%20Transpose%20Weights%20(CRQ%200039).mp4\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果我们从输入和权重的角度来看这一点，我们需要对每个输入和所有组合中设置的权重进行点积。点积取第一个数组的行和第二个数组的列，但目前两个数组中的数据都是行对齐的。转置第二个数组将数据整形为列对齐。输入和转置权重的矩阵乘积将生成包含我们需要计算的所有原子点乘积的矩阵。所得矩阵由对每个输入样本执行操作后所有神经元的输出组成："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video(\n",
    "    \"http://aimaksen.bslience.cn/nnf/Matrix%20Product%20Row%20Column%20Vector%20(GJW%200034).mp4\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们提到，np-dot（）的第二个参数将是我们的转置权重，因此第一个将是输入，但之前权重是第一个参数。我们在这里改变了。以前，我们使用单个数据样本（向量）对神经元输出进行建模，但现在我们在对一批数据建模层行为时向前迈出了一步。我们可以保留当前的参数顺序，但是，正如我们很快就会了解到的那样，与神经元及其输出逐样本的列表相比，由每个样本的层输出列表组成的结果更有用。我们希望得到的数组与样本相关，而不是与神经元相关，因为我们将进一步通过网络传递这些样本，下一层将需要一批输入。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们现在可以使用NumPy编写此解决方案。我们可以对普通的Python列表执行np-dot（），因为NumPy会在内部将它们转换为矩阵。我们自己转换权重，首先执行转置操作，代码中的T，因为普通的Python列表不支持它。说到偏差，我们不需要出于同样的原因将它变成NumPy数组——NumPy将在内部做到这一点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不过，偏差是一个列表，所以它们是一个一维数组，作为NumPy数组。将该偏置向量加到矩阵（在这种情况下为点积）的工作原理与我们前面描述的矩阵和向量的点积类似；偏置向量将被添加到矩阵的每个行向量。\n",
    "由于矩阵乘积结果的每一列都是一个神经元的输出，向量将被加到每一行向量上，第一个偏置将被加到这些向量的第一个元素上，从第二个到第二个，等等。这就是我们需要的-每个神经元的偏置需要被加到该神经元在所有输入向量（样本）上执行的所有结果上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.8    1.21   2.385]\n",
      " [ 8.9   -1.81   0.2  ]\n",
      " [ 1.41   1.051  0.026]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "inputs = [[1.0, 2.0, 3.0, 2.5], [2.0, 5.0, -1.0, 2.0], [-1.5, 2.7, 3.3, -0.8]]\n",
    "weights = [[0.2, 0.8, -0.5, 1.0], [0.5, -0.91, 0.26, -0.5], [-0.26, -0.27, 0.17, 0.87]]\n",
    "biases = [2.0, 3.0, 0.5]\n",
    "layer_outputs = np.dot(inputs, np.array(weights).T) + biases\n",
    "print(layer_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如您所见，我们的神经网络接收一组样本（输入）并输出一组预测。如果你使用过任何一个深度学习库，这就是为什么你会传入一个输入列表（即使它只是一个功能集），并返回一个预测列表，即使只有一个预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video(\n",
    "    \"http://aimaksen.bslience.cn/nnf/Matrix%20Product%20Addition%20Inputs%20Weights%20Biases%20(QTY%200035).mp4\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
